"""
PROTOTYPE

This script involves processing computed genome attributes and organizing them into a structured format suitable
for importing into ArangoDB. The resulting JSON file will be used to update (overwrite/insert) the database with the parsed
genome attribute data.

Note: If the ArangoDB collection has been previously created using a JSON file generated by the genome attributes
      loader script and if you want to replace the data created by that loader in ArangoDB, it is crucial to ensure
      that the arguments "--load_ver" and "--kbase_collection" are consistent with the ones used in the genome
      attributes loader script in order to ensure that the same key is generated for the corresponding Arango document.

usage: parse_computed_genome_attribs.py [-h] --load_ver LOAD_VER --kbase_collection KBASE_COLLECTION
                                        [--tools TOOLS [TOOLS ...]] [--root_dir ROOT_DIR] [-o OUTPUT]

options:
  -h, --help            show this help message and exit

required named arguments:
  --load_ver LOAD_VER   KBase load version (e.g. r207.kbase.1).
  --kbase_collection KBASE_COLLECTION
                        KBase collection identifier name.

optional arguments:
  --tools TOOLS [TOOLS ...]
                        Extract results from tools. (default: retrieve all available sub-directories in the [load_ver]
                        directory)
  --root_dir ROOT_DIR   Root directory for the collections project. (default: /global/cfs/cdirs/kbase/collections)
  -o OUTPUT, --output OUTPUT
                        output JSON file path

"""
import argparse
import copy
import os
import sys

import pandas as pd

from src.loaders.common import loader_common_names
from src.loaders.genome_attributes.loader_helper import convert_to_json, init_genome_atrri_doc, merge_docs

# Default result file name for parsed computed genome attributes data for arango import.
# Collection and load version information will be prepended to this file name.
COMPUTED_GENOME_ATTR_FILE = "computed_genome_attribs.json"

# The following features will be extracted from the CheckM2 result quality_report.tsv file as computed genome attributes
# If empty, select all available fields
SELECTED_CHECKM2_FEATURES = {'Completeness', 'Contamination'}

# The following features will be extracted from the GTDB-TK summary file
# ('gtdbtk.ar53.summary.tsv' or 'gtdbtk.bac120.summary.tsv') as computed genome attributes
# If empty, select all available fields
SELECTED_GTDBTK_SUMMARY_FEATURES = {}


def _locate_dir(root_dir, kbase_collection, load_ver, check_exists=False, tool=''):
    result_dir = os.path.join(root_dir, loader_common_names.COLLECTION_DATA_DIR, kbase_collection, load_ver, tool)

    if check_exists and not (os.path.exists(result_dir) and os.path.isdir(result_dir)):
        raise ValueError(f"Result directory for computed genome attributes of "
                         f"KBase Collection: {kbase_collection} and Load Version: {load_ver} could not be found.")

    return result_dir


def _read_tsv_as_df(file_path, features, genome_id_col=None):
    # Retrieve the desired fields from a TSV file and return the data in a dataframe

    selected_cols = copy.deepcopy(features) if features else None

    if selected_cols and genome_id_col:
        selected_cols.add(genome_id_col)

    df = pd.read_csv(file_path, sep='\t', keep_default_na=False, usecols=selected_cols)

    return df


def _create_doc(row, kbase_collection, load_version, genome_id, features, prefix):
    # Select specific columns and prepare them for import into Arango

    # NOTE: The selected column names will have a prefix added to them if pre_fix is not empty.

    doc = init_genome_atrri_doc(kbase_collection, load_version, genome_id)

    # distinguish the selected fields from the original metadata by adding a common prefix to their names
    if features:
        doc.update(row[list(features)].rename(lambda x: prefix + '_' + x if prefix else x).to_dict())
    else:
        doc.update(row.rename(lambda x: prefix + '_' + x if prefix else x).to_dict())

    return doc


def _row_to_doc(row, kbase_collection, load_version, features, tool_genome_map, genome_id_col, prefix):
    # Transforms a row from tool result file into ArangoDB collection document

    try:
        genome_id = tool_genome_map[row[genome_id_col]]
    except KeyError as e:
        raise ValueError('Unable to find genome ID') from e

    doc = _create_doc(row, kbase_collection, load_version, genome_id, features, prefix)

    return doc


def _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver, tool_file_name, features, genome_id_col,
                      prefix=''):
    # process the output file generated by the tool (checkm2, gtdb-tk, etc) to create a format suitable for importing
    # into ArangoDB
    # NOTE: If the tool result file does not exist, return an empty dictionary.

    batch_result_dir = os.path.join(result_dir, str(batch_dir))

    # retrieve and process the genome metadata file
    metadata_file = os.path.join(batch_result_dir, loader_common_names.GENOME_METADATA_FILE)
    try:
        meta_df = pd.read_csv(metadata_file, sep='\t', names=['tool_identifier', 'genome_id', 'source_file_path'])
    except Exception as e:
        raise ValueError('Unable to retrieve the genome metadata file') from e
    tool_genome_map = dict(zip(meta_df.tool_identifier, meta_df.genome_id))

    tool_file = os.path.join(result_dir, str(batch_dir), tool_file_name)
    docs = dict()
    if os.path.exists(tool_file):
        df = _read_tsv_as_df(tool_file, features, genome_id_col=genome_id_col)
        docs = df.apply(_row_to_doc, args=(kbase_collection, load_ver, features, tool_genome_map,
                                           genome_id_col, prefix), axis=1).to_list()

    return docs


def gtdb_tk(root_dir, kbase_collection, load_ver):
    """
    Parse and format result files generated by the GTDB-TK tool.
    """
    gtdb_tk_docs = list()

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, tool='gtdb_tk')

    # Get the list of directories for batches
    batch_dirs = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]

    summary_files = ['gtdbtk.ar53.summary.tsv', 'gtdbtk.bac120.summary.tsv']
    genome_id_col = 'user_genome'
    for batch_dir in batch_dirs:
        summary_file_exists = False

        for tool_file_name in summary_files:
            docs = _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver,
                                     tool_file_name, SELECTED_GTDBTK_SUMMARY_FEATURES, genome_id_col)

            if docs:
                summary_file_exists = True
                gtdb_tk_docs.extend(docs)

        if not summary_file_exists:
            raise ValueError(f'Unable to process the computed genome attributes for gtdb-tk in the specified '
                             f'directory {batch_dir}.')

    return gtdb_tk_docs


def checkm2(root_dir, kbase_collection, load_ver):
    """
    Parse and formate result files generated by the CheckM2 tool.
    """
    checkm2_docs = list()

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, tool='checkm2')

    # Get the list of directories for batches
    batch_dirs = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]

    tool_file_name, genome_id_col = 'quality_report.tsv', 'Name'
    for batch_dir in batch_dirs:
        docs = _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver,
                                 tool_file_name, SELECTED_CHECKM2_FEATURES, genome_id_col)

        if not docs:
            raise ValueError(f'Unable to process the computed genome attributes for checkm2 in the specified '
                             f'directory {batch_dir}.')

        checkm2_docs.extend(docs)

    return checkm2_docs


def main():
    parser = argparse.ArgumentParser(
        description='PROTOTYPE - Generate a JSON file for importing into ArangoDB by parsing computed '
                    'genome attributes.')
    required = parser.add_argument_group('required named arguments')
    optional = parser.add_argument_group('optional arguments')

    # Required flag arguments
    required.add_argument(f'--{loader_common_names.LOAD_VER_ARG_NAME}', required=True, type=str,
                          help=loader_common_names.LOAD_VER_DESCR)

    required.add_argument(f'--{loader_common_names.KBASE_COLLECTION_ARG_NAME}', required=True, type=str,
                          help=loader_common_names.KBASE_COLLECTION_DESCR)

    # Optional arguments
    optional.add_argument('--tools', type=str, nargs='+',
                          help=f'Extract results from tools. '
                               f'(default: retrieve all available sub-directories in the '
                               f'[{loader_common_names.LOAD_VER_ARG_NAME}] directory)')
    optional.add_argument('--root_dir', type=str, default=loader_common_names.ROOT_DIR,
                          help=f'Root directory for the collections project. (default: {loader_common_names.ROOT_DIR})')
    optional.add_argument("-o", "--output", type=argparse.FileType('w'),
                          help=f"output JSON file path")
    args = parser.parse_args()

    # Close the output file, as the file name will only be referenced later in the code.
    if args.output and not args.output.closed:
        args.output.close()

    (tools,
     load_ver,
     kbase_collection,
     root_dir) = (args.tools, getattr(args, loader_common_names.LOAD_VER_ARG_NAME),
                  getattr(args, loader_common_names.KBASE_COLLECTION_ARG_NAME), args.root_dir)

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, check_exists=True)

    executed_tools = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]
    if not executed_tools:
        raise ValueError(f'Cannot find any tool result folders in {result_dir}')

    tools = executed_tools if not tools else tools
    if set(tools) - set(executed_tools):
        raise ValueError(f'Please ensure that all tools have been successfully executed. '
                         f'Only the following tools have already been run: {executed_tools}')

    docs = list()
    for tool in tools:
        try:
            parse_ops = getattr(sys.modules[__name__], tool)
        except AttributeError as e:
            raise ValueError(f'Please implement parsing method for: [{tool}]') from e

        docs.extend(parse_ops(root_dir, kbase_collection, load_ver))

    docs = merge_docs(docs, '_key')

    output = args.output.name if args.output else f'{kbase_collection}_{load_ver}_{COMPUTED_GENOME_ATTR_FILE}'

    with open(output, 'w') as genome_attribs_json:
        convert_to_json(docs, genome_attribs_json)


if __name__ == "__main__":
    main()
