"""
PROTOTYPE

This script involves processing tool result files and organizing them into a structured format suitable
for importing into ArangoDB. The resulting JSON file will be used to update (overwrite/insert) the database with the
parsed data.

Note: If the ArangoDB collection has been previously created using a JSON file generated by the genome attributes
      loader script and if you want to replace the data created by that loader in ArangoDB, it is crucial to ensure
      that the arguments "--load_ver" and "--kbase_collection" are consistent with the ones used in the genome
      attributes loader script in order to ensure that the same key is generated for the corresponding Arango document.

usage: parse_tool_results.py [-h] --load_ver LOAD_VER --kbase_collection KBASE_COLLECTION
                                        [--tools TOOLS [TOOLS ...]] [--root_dir ROOT_DIR] [-o OUTPUT]

options:
  -h, --help            show this help message and exit

required named arguments:
  --load_ver LOAD_VER   KBase load version (e.g. r207.kbase.1).
  --kbase_collection KBASE_COLLECTION
                        KBase collection identifier name.

optional arguments:
  --tools TOOLS [TOOLS ...]
                        Extract results from tools. (default: retrieve all available sub-directories in the [load_ver]
                        directory)
  --root_dir ROOT_DIR   Root directory for the collections project. (default: /global/cfs/cdirs/kbase/collections)

"""
import argparse
import copy
import os
import sys
from typing import Dict, Union, List, Any

import pandas as pd

import src.common.storage.collection_and_field_names as names
from src.common.storage.db_doc_conversions import collection_data_id_key, collection_load_version_key
from src.loaders.common import loader_common_names
from src.loaders.common.loader_helper import convert_to_json, init_genome_atrri_doc, merge_docs
from src.service.data_products.heatmap_common_models import HeatMapMeta, ColumnInformation, ColumnCategory, Cell, \
    HeatMapRow, ColumnType

# Default result file name for parsed computed genome attributes data for arango import.
# Collection, load version and tools name will be prepended to this file name.
COMPUTED_GENOME_ATTR_FILE = "computed_genome_attribs.jsonl"

# Default result file name for parsed heatmap data for arango import.
# Collection, load version and tools name will be prepended to this file name.
HEATMAP_FILE = "heatmap_data.jsonl"

# The following features will be extracted from the CheckM2 result quality_report.tsv file as computed genome attributes
# If empty, select all available fields
SELECTED_CHECKM2_FEATURES = {'Completeness', 'Contamination'}

# The following features will be extracted from the GTDB-TK summary file
# ('gtdbtk.ar53.summary.tsv' or 'gtdbtk.bac120.summary.tsv') as computed genome attributes
# If empty, select all available fields
SELECTED_GTDBTK_SUMMARY_FEATURES = {}

# tools result will be parsed as computed genome attributes
GENOME_ATTR_TOOLS = ['checkm2', 'gtdb_tk']
# tool result will be parsed as headmap data
HEATMAP_TOOLS = ['microtrait']

# The following features will be extracted from the MicroTrait result file as heatmap data
_MICROTRAIT_TRAIT_NAME = 'microtrait_trait-name'  # unique identifier for a trait globally
_MICROTRAIT_TRAIT_DISPLAYNAME_SHORT = 'microtrait_trait-displaynameshort'
_MICROTRAIT_TRAIT_DISPLAYNAME_LONG = 'microtrait_trait-displaynamelong'
_MICROTRAIT_TRAIT_VALUE = 'microtrait_trait-value'

# The following features are used to create the heatmap metadata and rows
_SYS_TRAIT_ID = 'trait_id'  # unique identifier for a trait
_SYS_TRAIT_INDEX = 'trait_index'  # index of the trait
_SYS_TRAIT_NAME = 'trait_name'  # name of the trait
_SYS_TRAIT_DESCRIPTION = 'trait_description'  # description of the trait
_SYS_TRAIT_CATEGORY = 'trait_category'  # category of the trait
_SYS_TRAIT_VALUE = 'trait_value'  # value of the trait

_SYS_DEFAULT_TRAIT_VALUE = 0  # default value for a trait if the value is missing/not available

# The map between the MicroTrait trait names and the corresponding system trait names
# Use the microtrait_trait-name column as the unique identifier for a trait globally,
# the microtrait_trait-displaynameshort column as the column name,
# microtrait_trait-displaynamelong column as the column description, and
# microtrait_trait-value as the cell value
_MICROTRAIT_TO_SYS_TRAIT_MAP = {
    _MICROTRAIT_TRAIT_NAME: _SYS_TRAIT_ID,
    _MICROTRAIT_TRAIT_DISPLAYNAME_SHORT: _SYS_TRAIT_NAME,
    _MICROTRAIT_TRAIT_DISPLAYNAME_LONG: _SYS_TRAIT_DESCRIPTION,
    _MICROTRAIT_TRAIT_VALUE: _SYS_TRAIT_VALUE
}


def _locate_dir(root_dir, kbase_collection, load_ver, check_exists=False, tool=''):
    result_dir = os.path.join(root_dir, loader_common_names.COLLECTION_DATA_DIR, kbase_collection, load_ver, tool)

    if check_exists and not (os.path.exists(result_dir) and os.path.isdir(result_dir)):
        raise ValueError(f"Result directory for computed genome attributes of "
                         f"KBase Collection: {kbase_collection} and Load Version: {load_ver} could not be found.")

    return result_dir


def _read_tsv_as_df(file_path, features, genome_id_col=None):
    # Retrieve the desired fields from a TSV file and return the data in a dataframe

    selected_cols = copy.deepcopy(features) if features else None

    if selected_cols and genome_id_col:
        selected_cols.add(genome_id_col)

    df = pd.read_csv(file_path, sep='\t', keep_default_na=False, usecols=selected_cols)

    return df


def _create_doc(row, kbase_collection, load_version, genome_id, features, prefix):
    # Select specific columns and prepare them for import into Arango

    # NOTE: The selected column names will have a prefix added to them if pre_fix is not empty.

    doc = init_genome_atrri_doc(kbase_collection, load_version, genome_id)

    # distinguish the selected fields from the original metadata by adding a common prefix to their names
    if features:
        doc.update(row[list(features)].rename(lambda x: prefix + '_' + x if prefix else x).to_dict())
    else:
        doc.update(row.rename(lambda x: prefix + '_' + x if prefix else x).to_dict())

    return doc


def _row_to_doc(row, kbase_collection, load_version, features, tool_genome_map, genome_id_col, prefix):
    # Transforms a row from tool result file into ArangoDB collection document

    try:
        genome_id = tool_genome_map[row[genome_id_col]]
    except KeyError as e:
        raise ValueError('Unable to find genome ID') from e

    doc = _create_doc(row, kbase_collection, load_version, genome_id, features, prefix)

    return doc


def _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver, tool_file_name, features, genome_id_col,
                      prefix=''):
    # process the output file generated by the tool (checkm2, gtdb-tk, etc) to create a format suitable for importing
    # into ArangoDB
    # NOTE: If the tool result file does not exist, return an empty dictionary.

    batch_result_dir = os.path.join(result_dir, str(batch_dir))

    # retrieve and process the genome metadata file
    metadata_file = os.path.join(batch_result_dir, loader_common_names.GENOME_METADATA_FILE)
    try:
        meta_df = pd.read_csv(metadata_file, sep='\t', names=['tool_identifier', 'genome_id', 'source_file_path'])
    except Exception as e:
        raise ValueError('Unable to retrieve the genome metadata file') from e
    tool_genome_map = dict(zip(meta_df.tool_identifier, meta_df.genome_id))

    tool_file = os.path.join(result_dir, str(batch_dir), tool_file_name)
    docs = dict()
    if os.path.exists(tool_file):
        df = _read_tsv_as_df(tool_file, features, genome_id_col=genome_id_col)
        docs = df.apply(_row_to_doc, args=(kbase_collection, load_ver, features, tool_genome_map,
                                           genome_id_col, prefix), axis=1).to_list()

    return docs


def _get_batch_dirs(result_dir):
    # Get the list of directories for batches

    batch_dirs = [d for d in os.listdir(result_dir)
                  if os.path.isdir(os.path.join(result_dir, d))
                  and d.startswith(loader_common_names.COMPUTE_OUTPUT_PREFIX)]

    return batch_dirs


def gtdb_tk(root_dir, kbase_collection, load_ver):
    """
    Parse and format result files generated by the GTDB-TK tool.
    """
    gtdb_tk_docs = list()

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, tool='gtdb_tk')
    batch_dirs = _get_batch_dirs(result_dir)

    summary_files = ['gtdbtk.ar53.summary.tsv', 'gtdbtk.bac120.summary.tsv']
    genome_id_col = 'user_genome'
    for batch_dir in batch_dirs:
        summary_file_exists = False

        for tool_file_name in summary_files:
            docs = _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver,
                                     tool_file_name, SELECTED_GTDBTK_SUMMARY_FEATURES, genome_id_col)

            if docs:
                summary_file_exists = True
                gtdb_tk_docs.extend(docs)

        if not summary_file_exists:
            raise ValueError(f'Unable to process the computed genome attributes for gtdb-tk in the specified '
                             f'directory {batch_dir}.')

    return gtdb_tk_docs


def checkm2(root_dir, kbase_collection, load_ver):
    """
    Parse and formate result files generated by the CheckM2 tool.
    """
    checkm2_docs = list()

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, tool='checkm2')
    batch_dirs = _get_batch_dirs(result_dir)

    tool_file_name, genome_id_col = 'quality_report.tsv', 'Name'
    for batch_dir in batch_dirs:
        docs = _read_tool_result(result_dir, batch_dir, kbase_collection, load_ver,
                                 tool_file_name, SELECTED_CHECKM2_FEATURES, genome_id_col)

        if not docs:
            raise ValueError(f'Unable to process the computed genome attributes for checkm2 in the specified '
                             f'directory {batch_dir}.')

        checkm2_docs.extend(docs)

    return checkm2_docs


def _process_trait(row: Dict[str, Union[str, float, int, bool]],
                   traits_meta: Dict[str, Dict[str, str]],
                   traits_val: Dict[str, List[Dict[str, Union[int, float, bool]]]],
                   data_id: str, ):
    # Process a row from the trait file and update the global traits metadata and value lists accordingly

    _append_or_check_trait(traits_meta, row[_SYS_TRAIT_ID], row[_SYS_TRAIT_NAME], row[_SYS_TRAIT_DESCRIPTION],
                           row[_SYS_TRAIT_CATEGORY])
    _append_trait_val(traits_meta, traits_val, row[_SYS_TRAIT_ID], row[_SYS_TRAIT_VALUE], data_id)


def _append_trait_val(
        traits_meta: Dict[str, Dict[str, str]],
        traits_val: Dict[str, List[Dict[str, Union[int, float, bool]]]],
        trait_id: str,
        trait_value: float | int | bool,
        data_id: str):
    # Append a trait value to the global traits value list

    try:
        trait_index = traits_meta[trait_id][_SYS_TRAIT_INDEX]
    except KeyError as e:
        raise ValueError(f'Unable to find trait ID {trait_id}') from e

    if data_id not in traits_val:
        traits_val[data_id] = list()

    traits_val[data_id].append({_SYS_TRAIT_INDEX: trait_index,  # used as column index in the heatmap
                                _SYS_TRAIT_VALUE: trait_value})


def _append_or_check_trait(
        global_traits_meta: Dict[str, Dict[str, str]],
        trait_id: str,
        trait_name: str,
        trait_description: str,
        trait_category: str):
    # Append a new trait to the global traits dictionary or check if the trait information is consistent

    if trait_id not in global_traits_meta:
        # add new trait
        global_traits_meta[trait_id] = {
            _SYS_TRAIT_INDEX: len(global_traits_meta),  # used as column index in the heatmap
            _SYS_TRAIT_NAME: trait_name,
            _SYS_TRAIT_DESCRIPTION: trait_description,
            _SYS_TRAIT_CATEGORY: trait_category
        }
    else:
        # check if the trait information is consistent
        existing_trait = global_traits_meta[trait_id]
        if (existing_trait[_SYS_TRAIT_NAME] != trait_name or
                existing_trait[_SYS_TRAIT_DESCRIPTION] != trait_description or
                existing_trait[_SYS_TRAIT_CATEGORY] != trait_category):
            raise ValueError(f'Inconsistent trait information for trait {trait_id}')


def _parse_categories(traits_meta: Dict[str, Dict[str, str]]) -> List[ColumnCategory]:
    # Parse the trait categories from the global traits dictionary

    categories = {}
    # loop over each trait in traits_meta
    for trait in traits_meta.values():
        trait_category = trait[_SYS_TRAIT_CATEGORY]

        # if the trait_category is not available, create a new ColumnCategory object with empty columns
        if trait_category not in categories:
            categories[trait_category] = ColumnCategory(category=trait_category, columns=[])

        # add the trait to the appropriate category's list of columns
        categories[trait_category].columns.append(ColumnInformation(id=str(trait[_SYS_TRAIT_INDEX]),
                                                                    name=trait[_SYS_TRAIT_NAME],
                                                                    description=trait[_SYS_TRAIT_DESCRIPTION],
                                                                    type=ColumnType.COUNT.value))

    return list(categories.values())


def _is_float_int(num: Any) -> bool:
    # Check if a number is an integer or a float with an integer value
    return isinstance(num, int) or (isinstance(num, float) and num.is_integer())


def _append_cell(
        heatmap_row: HeatMapRow,
        trait_idx: int,
        cell_count: int,
        min_value: float | int,
        max_value: float | int,
        trait_val: float | int | bool = 0) -> (float | int, float | int):
    # Append a cell to the heatmap row and return the global min and max values

    cell = Cell(celid=str(cell_count), colid=str(trait_idx), val=trait_val)
    heatmap_row.cells.append(cell)

    return min(min_value, trait_val), max(max_value, trait_val)


def _parse_rows(
        traits_meta: Dict[str, Dict[str, Union[int, str]]],
        traits_val: Dict[str, List[Dict[str, Union[int, float, bool]]]],
        ensure_ints: bool = False) -> (List[HeatMapRow], Union[float, int], Union[float, int]):
    min_value, max_value, cell_count = float('inf'), float('-inf'), 0
    heatmap_rows, trait_idxs = [], set([trait[_SYS_TRAIT_INDEX] for trait in traits_meta.values()])
    for data_id, traits_val_list in traits_val.items():
        heatmap_row = HeatMapRow(kbase_id=data_id, cells=[])
        visited_traits = set()
        for trait_val_info in traits_val_list:
            trait_idx, trait_val = trait_val_info[_SYS_TRAIT_INDEX], trait_val_info[_SYS_TRAIT_VALUE]
            visited_traits.add(trait_idx)
            if ensure_ints:
                if not _is_float_int(trait_val):
                    raise ValueError(f'Trait value {trait_val} does not hold an integer value')
                trait_val = int(trait_val)
            min_value, max_value = _append_cell(heatmap_row, trait_idx, cell_count, min_value, max_value,
                                                trait_val=trait_val)
            cell_count += 1

        # fill in missing trait values with 0s for all cells
        missing_trait_idxs = trait_idxs - visited_traits
        for missing_trait_idx in missing_trait_idxs:
            min_value, max_value = _append_cell(heatmap_row, missing_trait_idx, cell_count, min_value, max_value,
                                                trait_val=_SYS_DEFAULT_TRAIT_VALUE)
            cell_count += 1

        heatmap_rows.append(heatmap_row)

    return heatmap_rows, min_value, max_value


def _create_heatmap_objs(
        traits_meta: Dict[str, Dict[str, str]],
        traits_val: Dict[str, List[Dict[str, Union[int, float, bool]]]]) -> (HeatMapMeta, List[HeatMapRow]):
    # Create the HeatMapMeta and list of HeatMapRow from parsed trait metadata and values

    heatmap_rows, min_value, max_value = _parse_rows(traits_meta, traits_val, ensure_ints=True)
    categories = _parse_categories(traits_meta)
    heatmap_meta = HeatMapMeta(categories=categories, min_value=min_value, max_value=max_value)

    return heatmap_meta, heatmap_rows


def microtrait(root_dir, kbase_collection, load_ver):
    """
    Parse and formate result files as heatmap data generated by the MicroTrait tool.
    """

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, tool='microtrait')
    batch_dirs = _get_batch_dirs(result_dir)

    traits_meta, traits_val = dict(), dict()
    for batch_dir in batch_dirs:
        data_ids = [item for item in os.listdir(os.path.join(result_dir, batch_dir)) if
                    os.path.isdir(os.path.join(result_dir, batch_dir, item))]
        for data_id in data_ids:
            data_dir = os.path.join(result_dir, batch_dir, data_id)
            trait_count_file = os.path.join(data_dir, loader_common_names.TRAIT_COUNTS_FILE)
            selected_cols = [_MICROTRAIT_TRAIT_NAME,
                             _MICROTRAIT_TRAIT_DISPLAYNAME_SHORT,
                             _MICROTRAIT_TRAIT_DISPLAYNAME_LONG,
                             _MICROTRAIT_TRAIT_VALUE]
            trait_df = pd.read_csv(trait_count_file, usecols=selected_cols)

            # Extract the substring of the 'microtrait_trait-displaynamelong' column before the last colon character
            # and assign it to a new 'category' column in the DataFrame
            trait_df[_SYS_TRAIT_CATEGORY] = trait_df[_MICROTRAIT_TRAIT_DISPLAYNAME_LONG].str.rsplit(':', 1).str[0]

            trait_df = trait_df.rename(columns=_MICROTRAIT_TO_SYS_TRAIT_MAP)
            trait_df.apply(_process_trait, args=(traits_meta, traits_val, data_id), axis=1).to_list()

    heatmap_meta, heatmap_rows = _create_heatmap_objs(traits_meta, traits_val)
    heatmap_meta_dict, heatmap_rows_list = heatmap_meta.dict(), [row.dict() for row in heatmap_rows]

    # Add _key, collection id and load version to the heatmap metadata and rows
    heatmap_meta_dict.update({names.FLD_ARANGO_KEY: collection_load_version_key(kbase_collection, load_ver),
                              names.FLD_COLLECTION_ID: kbase_collection,
                              names.FLD_LOAD_VERSION: load_ver})
    heatmap_rows_list = [dict(row, **{
        names.FLD_ARANGO_KEY: collection_data_id_key(kbase_collection, load_ver, row[names.FLD_KBASE_ID]),
        names.FLD_COLLECTION_ID: kbase_collection,
        names.FLD_LOAD_VERSION: load_ver}) for row in heatmap_rows_list]
    return heatmap_meta_dict, heatmap_rows_list


def _process_heatmap_tools(heatmap_tools, root_dir, kbase_collection, load_ver):
    # parse result files generated by heatmap tools such as microtrait

    for tool in heatmap_tools:
        try:
            parse_ops = getattr(sys.modules[__name__], tool)
        except AttributeError as e:
            raise ValueError(f'Please implement parsing method for: [{tool}]') from e

        heatmap_meta_dict, heatmap_rows_list = parse_ops(root_dir, kbase_collection, load_ver)

        meta_output = f'{kbase_collection}_{load_ver}_{tool}_meta_{HEATMAP_FILE}'
        rows_output = f'{kbase_collection}_{load_ver}_{tool}_rows_{HEATMAP_FILE}'

        with open(meta_output, 'w') as meta_output_json:
            convert_to_json([heatmap_meta_dict], meta_output_json)

        with open(rows_output, 'w') as rows_output_json:
            convert_to_json(heatmap_rows_list, rows_output_json)


def _process_genome_attri_tools(genome_attr_tools, root_dir, kbase_collection, load_ver):
    # parse result files generated by genome attribute tools such as checkm2, gtdb-tk, etc

    if not genome_attr_tools:
        return

    docs = list()
    for tool in genome_attr_tools:
        try:
            parse_ops = getattr(sys.modules[__name__], tool)
        except AttributeError as e:
            raise ValueError(f'Please implement parsing method for: [{tool}]') from e

        docs.extend(parse_ops(root_dir, kbase_collection, load_ver))

    docs = merge_docs(docs, '_key')

    output = f'{kbase_collection}_{load_ver}_{"_".join(genome_attr_tools)}_{COMPUTED_GENOME_ATTR_FILE}'

    with open(output, 'w') as genome_attribs_json:
        convert_to_json(docs, genome_attribs_json)


def main():
    parser = argparse.ArgumentParser(
        description='PROTOTYPE - Generate a JSON file for importing into ArangoDB by parsing computed '
                    'genome attributes.')
    required = parser.add_argument_group('required named arguments')
    optional = parser.add_argument_group('optional arguments')

    # Required flag arguments
    required.add_argument(f'--{loader_common_names.LOAD_VER_ARG_NAME}', required=True, type=str,
                          help=loader_common_names.LOAD_VER_DESCR)

    required.add_argument(f'--{loader_common_names.KBASE_COLLECTION_ARG_NAME}', required=True, type=str,
                          help=loader_common_names.KBASE_COLLECTION_DESCR)

    # Optional arguments
    optional.add_argument('--tools', type=str, nargs='+',
                          help=f'Extract results from tools. '
                               f'(default: retrieve all available sub-directories in the '
                               f'[{loader_common_names.LOAD_VER_ARG_NAME}] directory)')
    optional.add_argument('--root_dir', type=str, default=loader_common_names.ROOT_DIR,
                          help=f'Root directory for the collections project. (default: {loader_common_names.ROOT_DIR})')
    args = parser.parse_args()

    tools = args.tools
    load_ver = getattr(args, loader_common_names.LOAD_VER_ARG_NAME)
    kbase_collection = getattr(args, loader_common_names.KBASE_COLLECTION_ARG_NAME)
    root_dir = args.root_dir

    result_dir = _locate_dir(root_dir, kbase_collection, load_ver, check_exists=True)

    executed_tools = [d for d in os.listdir(result_dir) if os.path.isdir(os.path.join(result_dir, d))]
    if not executed_tools:
        raise ValueError(f'Cannot find any tool result folders in {result_dir}')

    tools = executed_tools if not tools else tools
    if set(tools) - set(executed_tools):
        raise ValueError(f'Please ensure that all tools have been successfully executed. '
                         f'Only the following tools have already been run: {executed_tools}')

    _process_genome_attri_tools(set(GENOME_ATTR_TOOLS).intersection(tools), root_dir, kbase_collection, load_ver)
    _process_heatmap_tools(set(HEATMAP_TOOLS).intersection(tools), root_dir, kbase_collection, load_ver)


if __name__ == "__main__":
    main()
